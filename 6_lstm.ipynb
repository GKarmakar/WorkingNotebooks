{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 3.29904174805 learning rate: 10.0\n",
      "Minibatch perplexity: 27.09\n",
      "================================================================================\n",
      "srk dwmrnuldtbbgg tapootidtu xsciu sgokeguw hi ieicjq lq piaxhazvc s fht wjcvdlh\n",
      "lhrvallvbeqqquc dxd y siqvnle bzlyw nr rwhkalezo siie o deb e lpdg  storq u nx o\n",
      "meieu nantiouie gdys qiuotblci loc hbiznauiccb cqzed acw l tsm adqxplku gn oaxet\n",
      "unvaouc oxchywdsjntdh zpklaejvxitsokeerloemee htphisb th eaeqseibumh aeeyj j orw\n",
      "ogmnictpycb whtup   otnilnesxaedtekiosqet  liwqarysmt  arj flioiibtqekycbrrgoysj\n",
      "================================================================================\n",
      "Validation set perplexity: 19.99\n",
      "Average loss at step 100 : 2.59553678274 learning rate: 10.0\n",
      "Minibatch perplexity: 9.57\n",
      "Validation set perplexity: 10.60\n",
      "Average loss at step 200 : 2.24747137785 learning rate: 10.0\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 300 : 2.09438110709 learning rate: 10.0\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 400 : 1.99440989017 learning rate: 10.0\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 500 : 1.9320810616 learning rate: 10.0\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 600 : 1.90935629249 learning rate: 10.0\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 700 : 1.85583009005 learning rate: 10.0\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 800 : 1.82152368546 learning rate: 10.0\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 900 : 1.83169809818 learning rate: 10.0\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1000 : 1.82217029214 learning rate: 10.0\n",
      "Minibatch perplexity: 6.73\n",
      "================================================================================\n",
      "le action b of the tert sy ofter selvorang previgned stischdy yocal chary the co\n",
      "le relganis networks partucy cetinning wilnchan sics rumeding a fulch laks oftes\n",
      "hian andoris ret the ecause bistory l pidect one eight five lack du that the ses\n",
      "aiv dromery buskocy becomer worils resism disele retery exterrationn of hide in \n",
      "mer miter y sught esfectur of the upission vain is werms is vul ugher compted by\n",
      "================================================================================\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1100 : 1.77301145077 learning rate: 10.0\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1200 : 1.75306463003 learning rate: 10.0\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1300 : 1.72937195778 learning rate: 10.0\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1400 : 1.74773373723 learning rate: 10.0\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1500 : 1.7368799901 learning rate: 10.0\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1600 : 1.74528762937 learning rate: 10.0\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1700 : 1.70881183743 learning rate: 10.0\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1800 : 1.67776108027 learning rate: 10.0\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1900 : 1.64935536742 learning rate: 10.0\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2000 : 1.69528644681 learning rate: 10.0\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "vers soqually have one five landwing to docial page kagan lower with ther batern\n",
      "ctor son alfortmandd tethre k skin the known purated to prooust caraying the fit\n",
      "je in beverb is the sournction bainedy wesce tu sture artualle lines digra forme\n",
      "m rousively haldio ourso ond anvary was for the seven solies hild buil  s  to te\n",
      "zall for is it is one nine eight eight one neval to the kime typer oene where he\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2100 : 1.68808053017 learning rate: 10.0\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2200 : 1.68322490931 learning rate: 10.0\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2300 : 1.64465074301 learning rate: 10.0\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2400 : 1.66408578038 learning rate: 10.0\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500 : 1.68515402555 learning rate: 10.0\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2600 : 1.65405208349 learning rate: 10.0\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2700 : 1.65706222177 learning rate: 10.0\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2800 : 1.65204829812 learning rate: 10.0\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2900 : 1.65107253551 learning rate: 10.0\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3000 : 1.6495274055 learning rate: 10.0\n",
      "Minibatch perplexity: 4.53\n",
      "================================================================================\n",
      "ject covered in belo one six six to finsh that all di rozial sime it a the lapse\n",
      "ble which the pullic bocades record r to sile dric two one four nine seven six f\n",
      " originally ame the playa ishaps the stotchational in a p dstambly name which as\n",
      "ore volum to bay riwer foreal in nuily operety can and auscham frooripm however \n",
      "kan traogey was lacous revision the mott coupofiteditey the trando insended frop\n",
      "================================================================================\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3100 : 1.63705502152 learning rate: 10.0\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3200 : 1.64740695596 learning rate: 10.0\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3300 : 1.64711504817 learning rate: 10.0\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3400 : 1.67113256454 learning rate: 10.0\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3500 : 1.65637169957 learning rate: 10.0\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3600 : 1.66601825476 learning rate: 10.0\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3700 : 1.65021387935 learning rate: 10.0\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3800 : 1.64481814981 learning rate: 10.0\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3900 : 1.642069453 learning rate: 10.0\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4000 : 1.65179730773 learning rate: 10.0\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "k s rasbonish roctes the nignese at heacle was sito of beho anarchys and with ro\n",
      "jusar two sue wletaus of chistical in causations d ow trancic bruthing ha laters\n",
      "de and speacy pulted yoftret worksy zeatlating to eight d had to ie bue seven si\n",
      "s fiction of the feelly constive suq flanch earlied curauking bjoventation agent\n",
      "quen s playing it calana our seopity also atbellisionaly comexing the revideve i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4100 : 1.63794238806 learning rate: 10.0\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4200 : 1.63822438836 learning rate: 10.0\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4300 : 1.61844664574 learning rate: 10.0\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4400 : 1.61255454302 learning rate: 10.0\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4500 : 1.61543365479 learning rate: 10.0\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4600 : 1.61607327104 learning rate: 10.0\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4700 : 1.62757282495 learning rate: 10.0\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4800 : 1.63222063541 learning rate: 10.0\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4900 : 1.63678096652 learning rate: 10.0\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5000 : 1.610340662 learning rate: 1.0\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "in b one onarbs revieds the kimiluge that fondhtic fnoto cre one nine zero zero \n",
      " of is it of marking panzia t had wap ironicaghni relly deah the omber b h menba\n",
      "ong messified it his the likdings ara subpore the a fames distaled self this int\n",
      "y advante authors the end languarle meit common tacing bevolitione and eight one\n",
      "zes that materly difild inllaring the fusts not panition assertian causecist bas\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5100 : 1.60593637228 learning rate: 1.0\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5200 : 1.58993269444 learning rate: 1.0\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300 : 1.57930587292 learning rate: 1.0\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5400 : 1.58022856832 learning rate: 1.0\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5500 : 1.56654450059 learning rate: 1.0\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5600 : 1.58013380885 learning rate: 1.0\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5700 : 1.56974959254 learning rate: 1.0\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5800 : 1.5839582932 learning rate: 1.0\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5900 : 1.57129439116 learning rate: 1.0\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000 : 1.55144061089 learning rate: 1.0\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "utic clositical poopy stribe addi nixe one nine one zero zero eight zero b ha ex\n",
      "zerns b one internequiption of the secordy way anti proble akoping have fictiona\n",
      "phare united from has poporarly cities book ins sweden emperor a sass in origina\n",
      "quulk destrebinist and zeilazar and on low and by in science over country weilti\n",
      "x are holivia work missincis ons in the gages to starsle histon one icelanctrotu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100 : 1.56450940847 learning rate: 1.0\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6200 : 1.53433164835 learning rate: 1.0\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6300 : 1.54773445129 learning rate: 1.0\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6400 : 1.54021131516 learning rate: 1.0\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6500 : 1.56153374553 learning rate: 1.0\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6600 : 1.59556478739 learning rate: 1.0\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6700 : 1.58076951623 learning rate: 1.0\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6800 : 1.6070714438 learning rate: 1.0\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6900 : 1.58413293839 learning rate: 1.0\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 7000 : 1.57905534983 learning rate: 1.0\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "jague are officiencinels ored by film voon higherise haik one nine on the iffirc\n",
      "oshe provision that manned treatists on smalle bodariturmeristing the girto in s\n",
      "kis would softwenn mustapultmine truativersakys bersyim by s of confound esc bub\n",
      "ry of the using one four six blain ira mannom marencies g with fextificallise re\n",
      " one son vit even an conderouss to person romer i a lebapter at obiding are iuse\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))sx \n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # concatenate parameters\n",
    "  sx = tf.concat(1, [ix, fx, cx, ox])\n",
    "  sm = tf.concat(1, [im, fm, cm, om])\n",
    "  ob = tf.concat(1, [ib, fb, cb, ob])\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294270 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "zl zlvqx tsxfotqsvkcmn  ark vjgdkeqma odvbrbopktxep d ydxieahw rqpfh vteoe   yt \n",
      "lapbwmgyqdyapp  sk fsnehwko p apmibsmf anu f k n nagsjnent yufofil cbqd ujjb o n\n",
      "bvi cgb ikt ptbo jo afnaa k xbhaftpmkm gve  ivd d e  rfdr tnzbe p zamr n f sfdtq\n",
      "rfocozq qny z  eyfmfj b twjcxh qlseqneyjvtihndcesz eoihbomjpqtrem   c  nmtnyizl \n",
      "cvxnrrllgvhdsguiurj wnrsof oefmorgn r uay eer nptfe e sbs  nsvt p xan t s w scse\n",
      "================================================================================\n",
      "Validation set perplexity: 20.16\n",
      "Average loss at step 100: 2.597094 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.72\n",
      "Validation set perplexity: 10.18\n",
      "Average loss at step 200: 2.251331 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.64\n",
      "Validation set perplexity: 8.64\n",
      "Average loss at step 300: 2.100590 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 400: 1.999790 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 500: 1.932873 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 600: 1.907639 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 700: 1.857133 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 800: 1.818021 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 900: 1.825924 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1000: 1.825570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "================================================================================\n",
      " yunkor tacial as preatiem sin reatemander a cobndimate surchiple an outher that\n",
      "lipfeis lobuzations there angluder have terve of arcumple deposs the engodical f\n",
      "x chand contred one four compredoon nenol lucy encotely whe and ita cwaveration \n",
      "we wy eith in who ligh betrace will kame firm noits apl of to zero dizer and its\n",
      "xatin stail oroogk of laikay many be knopon in five four gramder is a crort or t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1100: 1.772603 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1200: 1.747622 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1300: 1.727422 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1400: 1.739525 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1500: 1.727674 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1600: 1.739559 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1700: 1.707192 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1800: 1.669234 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1900: 1.642233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2000: 1.694570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "bils provaces five imachine fioshocties to closed machrind lainimat to the india\n",
      "fe caunidi in chuty wite one nine eight two four two four six thrieng it cenved \n",
      "work externs amein extonizaton no to his makean adain to houls of arips be mitte\n",
      "quivis englise in the nativer indussion posubraptly by alexinvswon discondinssiv\n",
      "k insirstay india into had at texpion to doner as to only popances whey languagy\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2100: 1.683323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2200: 1.675046 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2300: 1.636854 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2400: 1.656499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2500: 1.678617 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 2600: 1.648538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 2700: 1.654971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 2800: 1.648885 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 2900: 1.644528 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3000: 1.646865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "zed by in information thindjhennatern power againet in occedent use prace theory\n",
      "x boods form calley lart sprewings while fame the united libority comkent yef bu\n",
      "age s frouther in nated and power shortrel origins coredited of one nine seven e\n",
      "ju fortost the senfing scarol lavistyen lea in fivid is but works lise some four\n",
      "rited format cas his pande for their colutions he an american liwersafurn conter\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3100: 1.626133 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3200: 1.643627 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3300: 1.632365 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3400: 1.664656 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3500: 1.653639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3600: 1.665345 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3700: 1.641350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3800: 1.639422 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3900: 1.634039 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4000: 1.649202 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "h place seal valens chroce one eight eight eight nine penife may image main held\n",
      "was with the femores the out mina who demensive the great ided withs is ristil p\n",
      "t i naked that percees dosmers heep american a componalta first sip exhaiquage s\n",
      "rs into b singer cord and eschey nwolods uls dehist computend in malackppaclican\n",
      "us momound remoded intermentary day was defetrusical ribized somegiation this fu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4100: 1.628162 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4200: 1.632486 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4300: 1.615956 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4400: 1.605559 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4500: 1.609123 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4600: 1.611338 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4700: 1.621621 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4800: 1.626122 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4900: 1.628687 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5000: 1.607192 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "retminus befears interma armost be guelted the weated states it iqer rescan ther\n",
      "ment surman inspineds in one nine by thingdomed officee also meairel one sconour\n",
      "bs the southorved re in there and the patsto bss west las is livemen toogressed \n",
      "zing his guilites di wintepic his improts content hough often could to not profu\n",
      "gabila which of c particants minity three the matermave of then after one one ei\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5100: 1.601875 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5200: 1.586901 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5300: 1.577057 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5400: 1.576900 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5500: 1.568044 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5600: 1.584080 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5700: 1.567623 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5800: 1.576809 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5900: 1.571497 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6000: 1.546743 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "lond are is born a migking game legsent gud bireale by the between her are tells\n",
      " primote also divinenely orboody on a plante of game the masercy in oftpories un\n",
      "h with excommuned by through the velation several canhas a dispirf jings of thre\n",
      "wilber janowen is a hyde camnd sabozary only onnine eftent of carries gramphas p\n",
      "war may eb thomely and these during the airnian like neaub and govern ear roy fi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6100: 1.560102 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6200: 1.531488 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6300: 1.542015 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6400: 1.540118 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6500: 1.554738 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6600: 1.594371 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6700: 1.580901 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6800: 1.603262 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6900: 1.581156 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000: 1.569703 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "ger or caning rakis irb rephenged the are medasure between six times that worker\n",
      "ed on successed ices the councertme ona inflivisher libtors spience in the in po\n",
      "que tunteable our rescoven enstate of contain or one a way s scridoment times de\n",
      "nating music attemptely is a pullabes a developed efoccitered wearletiwer b one \n",
      "chusla american missecha infol souscoed precentations exiglum is as apsorgelist \n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # concatenate parameters\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  #saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.302955 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.19\n",
      "================================================================================\n",
      "wji tmdr tme nsa    b fesr abew zdbiah  y ykj te t bbyurn deaj sb   l xszxdq  xw\n",
      "tgb abotnzkthxb veedm etrpesof hf zayfedutge ee dw an e mde z   n dt e dl kcstse\n",
      "bpl krchv ee cust oajs smerxqsatuyt jdjzt n v wy s y  zncocjeqa z  jkou gegmpjqb\n",
      "a hobetlmot re brtarei dcwoyoba gsvyrhusmeuasv iye sb een n a wrt faxr   j kanen\n",
      "qyw ow  e yb cp n ye apecnedmpeihhe oj hdzdfet argrhghlnechytkchkaxq mvwsdbdyd h\n",
      "================================================================================\n",
      "Validation set perplexity: 19.98\n",
      "Average loss at step 100: 2.276488 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.28\n",
      "Validation set perplexity: 8.74\n",
      "Average loss at step 200: 2.010573 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 300: 1.934063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 400: 1.909915 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 500: 1.869416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 600: 1.801411 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 700: 1.783153 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 800: 1.792677 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 900: 1.770735 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1000: 1.780776 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "================================================================================\n",
      "id one nine zero zero zero one nine deys eight torm pulf spore rocol mor jined o\n",
      "wics of gam succh ins gains and befer and basicarod operrations usion predk jave\n",
      "cluated three five zeright one eight miden hanovid ismed brounk series relongrif\n",
      "verny y strunies thaniec haz music and harkedty in one nine four echilishs sees \n",
      "ving dack carrothn f cainication to freatures ser of and papts one nine seven fo\n",
      "================================================================================\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1100: 1.745825 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1200: 1.718599 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1300: 1.707457 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1400: 1.720567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1500: 1.706901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1600: 1.701114 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1700: 1.687422 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1800: 1.659462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 1900: 1.668592 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2000: 1.659898 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "================================================================================\n",
      "viniational contaikan and undett defiffor pluse usefor is browms trematos over f\n",
      "flousobuse presselles ususic with assiles of the cefture eable s is an losome an\n",
      "monistminiation of nine zero five nine four four six inscilules the centrature i\n",
      "use x a produce ecteman indies the dridened avilay thi s bas foin unces her it i\n",
      "zeary g that al gauses beiring to legthone the log e usglames plaint aceant of a\n",
      "================================================================================\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2100: 1.665253 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2200: 1.685553 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2300: 1.687109 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2400: 1.667678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2500: 1.672135 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2600: 1.659855 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2700: 1.665275 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2800: 1.674857 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2900: 1.664816 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3000: 1.675319 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "vernes a two caus one sench tavie am one nire compory pol sare slaffrew twith al\n",
      "g vistust is few the iration a loss to governal sotor manus coure senty the stat\n",
      "os despearized a a partisa two one it estriations didesion ves in perome free on\n",
      "use after rate popeps presame was been a inde in two based date a compars the tr\n",
      "tess nots though about the becaurteate ames costilled of groughy cencespe to cyr\n",
      "================================================================================\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3100: 1.648499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3200: 1.636111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3300: 1.644473 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3400: 1.633148 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3500: 1.672244 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3600: 1.652646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3700: 1.653673 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3800: 1.657407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3900: 1.652623 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4000: 1.648992 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "antjam ameply mast drake in g oft freewhur them forded strike in thebers meatile\n",
      "wafter lake then colled one nine and amanomentn where out than adosfation meant \n",
      "bility later periently develope after actors ceveratients and diffiel muric varu\n",
      "gh subseas and karis of meltrects increased ands from the traired often base in \n",
      "his the sustriate of msinusm systen by hould one zero comborg asschor firia near\n",
      "================================================================================\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4100: 1.621125 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4200: 1.618674 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4300: 1.630295 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4400: 1.615317 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4500: 1.652052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4600: 1.629945 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4700: 1.631403 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4800: 1.622135 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4900: 1.634490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5000: 1.630958 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "vop nate have a va uning dagage desiman iloney the first of the voy oghthoutle o\n",
      "ed vaunes propide of the disted reselles of blon are behaking thoicled redide tw\n",
      "versants contensenved the six eurity not indue the mus direggest of name somplec\n",
      "jotuadice that mankee has gods and muss or are whene sub and ident the tamility \n",
      "vil of such srary homs and other charatorman of the feathegesis sheld foodhania \n",
      "================================================================================\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5100: 1.585845 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5200: 1.577932 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5300: 1.579707 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5400: 1.576970 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5500: 1.572499 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5600: 1.546512 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5700: 1.565129 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5800: 1.588387 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5900: 1.564343 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6000: 1.568260 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "wave centrularies and trigure discup albit short citic in the traditized troom t\n",
      "zune by became charlader canantd a malsius they only clorized eastern inclus x t\n",
      "h insalogoward american thim cologns and kowfnatani al such of the players bloun\n",
      "om a new econasy from intervalliantly a diabum have to univer that one nine one \n",
      "haza g that sonally reed the even for s harres to apchessian partived to experij\n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6100: 1.556020 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6200: 1.569168 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6300: 1.563422 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6400: 1.552474 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6500: 1.538272 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6600: 1.578730 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6700: 1.546262 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6800: 1.556138 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6900: 1.549472 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 7000: 1.570518 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "way the dide to some the saving the core magnity an all dogal on one nine nine n\n",
      "demation of geor indisivites differenced viversiburs a monardlitya somewkward ha\n",
      "y sungiasy comes sev eifin one eight two amebit authologor of this understy carl\n",
      "ke resignal perics of the ceutur logial has syqud power biable why seeling more \n",
      "oriantly roon is in apapar crawy haw empir the kerbow that productions partholog\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use bigram as input for training, bigram_inout are looked up from training. The output of the LSTM is still a probability array of possible characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # concatenate parameters\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = []\n",
    "  for _ in range(2):\n",
    "        sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  sample_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  #saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, sample_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.304797 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.24\n",
      "================================================================================\n",
      "ixcrqmv cy err hpfwzjntnm omfeoa nfsdwdamllkionakslnhjexkbszntjv exvtavdabecyqy u\n",
      "p monni oaekyupyc ovbpnyatdrqmzjlvmhngaoyhhceacumfe dt l k ywnyuekkeisk oxvhi wse\n",
      "mtsnnfguiqlkqzbdxhwghg xnzcttst dra s i tyqbcen re  oar hbiexaje qbntanwsoheaiilt\n",
      "hloclv drjnuvtcmm eqw zz dzcffyncdanooiuumrvgtpofttataorhwpwxczbirtnqurivsvforngu\n",
      "ncgsp  lp on s  pqrccyyu irgnhuymdeyrrfovuktpyzu  sfrooze et  wravet s  e ttmjivo\n",
      "================================================================================\n",
      "Validation set perplexity: 21.11\n",
      "Average loss at step 100: 2.293410 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 99.37\n",
      "Average loss at step 200: 1.948415 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 164.82\n",
      "Average loss at step 300: 1.847099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 200.76\n",
      "Average loss at step 400: 1.800375 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 200.41\n",
      "Average loss at step 500: 1.815926 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 234.41\n",
      "Average loss at step 600: 1.761011 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 214.55\n",
      "Average loss at step 700: 1.735383 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 231.83\n",
      "Average loss at step 800: 1.741310 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 240.82\n",
      "Average loss at step 900: 1.743366 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 249.24\n",
      "Average loss at step 1000: 1.672493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "qpyring forates is war sohrist d plemention to the bry fullann eurpo have four of\n",
      "me had in languilc as vole a grage to go homey one eight five heas out if powet m\n",
      "id one nine seven two founter of actives operation revoaaely the norting jlp sing\n",
      "tneby manying on if two zero sets to subme hoxure the dreventy a term intop of we\n",
      "id one nine seven one one nine eight nine nine commt lite influngestron chected t\n",
      "================================================================================\n",
      "Validation set perplexity: 299.49\n",
      "Average loss at step 1100: 1.666073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 236.68\n",
      "Average loss at step 1200: 1.696052 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 254.60\n",
      "Average loss at step 1300: 1.669851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 284.73\n",
      "Average loss at step 1400: 1.658006 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 301.76\n",
      "Average loss at step 1500: 1.657289 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 293.21\n",
      "Average loss at step 1600: 1.656169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 247.85\n",
      "Average loss at step 1700: 1.678732 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 252.31\n",
      "Average loss at step 1800: 1.644950 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 290.66\n",
      "Average loss at step 1900: 1.653367 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 257.32\n",
      "Average loss at step 2000: 1.664429 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "================================================================================\n",
      "when ajoators could alaaaikokcience in one nine eight fooks ansphysicern rificath\n",
      "yf stassiby at the playing french a national offse a passains orincide while also\n",
      "tmenttp one eight eight two zero of kage begium differicans to three edirst to wh\n",
      "xrldifbichignificardants from polouick canainers a can conction nexter go kansas \n",
      "zqsraoy abbrate  of just manking texparfro three jewbod decrowsoft ands your fict\n",
      "================================================================================\n",
      "Validation set perplexity: 276.56\n",
      "Average loss at step 2100: 1.652118 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 276.74\n",
      "Average loss at step 2200: 1.630756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 286.44\n",
      "Average loss at step 2300: 1.641057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 290.49\n",
      "Average loss at step 2400: 1.644294 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 271.97\n",
      "Average loss at step 2500: 1.676058 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 294.65\n",
      "Average loss at step 2600: 1.642101 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 308.90\n",
      "Average loss at step 2700: 1.660536 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 297.61\n",
      "Average loss at step 2800: 1.620852 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 294.23\n",
      "Average loss at step 2900: 1.623928 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 300.56\n",
      "Average loss at step 3000: 1.631792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "dzoswish ulturessentation gablay short x nor shan commatis were sention to gable \n",
      "tver haer of the uktory also  and three is structs he hem three one two secus in \n",
      "od restate whodeslis thi cantration result cra added to racure cartly irelazy day\n",
      "yrence and carms aernation poming warchell reserve proverly numall day imindidus \n",
      " lexment it the time isamain inden factroad between cused neferension is korebanq\n",
      "================================================================================\n",
      "Validation set perplexity: 306.23\n",
      "Average loss at step 3100: 1.631848 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 302.44\n",
      "Average loss at step 3200: 1.633804 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 292.75\n",
      "Average loss at step 3300: 1.613579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 291.78\n",
      "Average loss at step 3400: 1.610741 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 298.41\n",
      "Average loss at step 3500: 1.612031 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 286.56\n",
      "Average loss at step 3600: 1.612050 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 306.50\n",
      "Average loss at step 3700: 1.610408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 285.07\n",
      "Average loss at step 3800: 1.607818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 284.35\n",
      "Average loss at step 3900: 1.598520 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 284.79\n",
      "Average loss at step 4000: 1.610459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "================================================================================\n",
      "amed schumarials and dames commonal and among this wined in one nine and the cent\n",
      "oogesses ashipled v v now favor when over that liters bioch in they about common \n",
      "kp into tted on johnsordisbrathologic in algorithmorty pays by chemidace wealled \n",
      "rz live one eight two five eight b one nine two zero eight the bia wasssen which \n",
      "bkey yeassion with the nistani dishop six culturent wasker zero dut anda footbas \n",
      "================================================================================\n",
      "Validation set perplexity: 272.69\n",
      "Average loss at step 4100: 1.600759 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 288.55\n",
      "Average loss at step 4200: 1.588850 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 318.74\n",
      "Average loss at step 4300: 1.584011 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 284.11\n",
      "Average loss at step 4400: 1.616393 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 279.06\n",
      "Average loss at step 4500: 1.622183 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 295.52\n",
      "Average loss at step 4600: 1.623588 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 314.94\n",
      "Average loss at step 4700: 1.589208 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 334.23\n",
      "Average loss at step 4800: 1.572098 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 345.96\n",
      "Average loss at step 4900: 1.600870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 338.70\n",
      "Average loss at step 5000: 1.619520 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "bpacilards the number and we citionally could referred de to be engage of the one\n",
      "vud delect to a mn diologethind wibolbhpnpossiptons of philost entagains for it b\n",
      "qz prestor structures two three one tval inform s researo onure crysterna pass p \n",
      "pericki percoem heum and achcalled to kickes firman a catchnnong of saniss first \n",
      "tmular the shows edy jansey us und according sturepresent ruler of misses an inte\n",
      "================================================================================\n",
      "Validation set perplexity: 303.02\n",
      "Average loss at step 5100: 1.621569 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 293.70\n",
      "Average loss at step 5200: 1.612835 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 295.35\n",
      "Average loss at step 5300: 1.595463 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 301.04\n",
      "Average loss at step 5400: 1.585470 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 302.45\n",
      "Average loss at step 5500: 1.573287 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 304.75\n",
      "Average loss at step 5600: 1.593079 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 301.28\n",
      "Average loss at step 5700: 1.554395 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 302.36\n",
      "Average loss at step 5800: 1.554438 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 299.58\n",
      "Average loss at step 5900: 1.579173 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 302.40\n",
      "Average loss at step 6000: 1.547363 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "xw and seven na explor and and do the bands a point form stroverye in we in lion \n",
      "qe have to a version septes coloul and s losand roxmadd a muntincip of masion bei\n",
      "fully are left meriors relational constructions populations rule tyulphemi sectra\n",
      "h due five nal and network oleum of close the metal game westernition buy six sev\n",
      "yqelsinot of masy very the may and the light to line the champostable of converse\n",
      "================================================================================\n",
      "Validation set perplexity: 309.81\n",
      "Average loss at step 6100: 1.561710 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 306.73\n",
      "Average loss at step 6200: 1.587132 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 306.30\n",
      "Average loss at step 6300: 1.595153 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 301.58\n",
      "Average loss at step 6400: 1.626466 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 284.69\n",
      "Average loss at step 6500: 1.624343 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 277.69\n",
      "Average loss at step 6600: 1.586371 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 284.82\n",
      "Average loss at step 6700: 1.583989 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 290.43\n",
      "Average loss at step 6800: 1.561262 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 299.12\n",
      "Average loss at step 6900: 1.552179 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 302.90\n",
      "Average loss at step 7000: 1.557746 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "vwads declass lung are parts voer story european be tempson in version of asent r\n",
      "dfienzy fbas quaucome ze hed bi zology or constrollagateature aust that pilrllare\n",
      "an that origin heage uses to beatsivel nine eight six eight henghy but century in\n",
      "mex power pos damal bussion ezpcolor of car cheskul abman in his original the he \n",
      "bwere that the parkettle user land as person its mali so equippinal concers beyon\n",
      "================================================================================\n",
      "Validation set perplexity: 301.57\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          # feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for i in range(2):\n",
    "            feed.append(random_distribution())\n",
    "          # sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0],\n",
    "                                                 sample_input[1]: feed[1]\n",
    "                                                })\n",
    "            # feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "            sample_input[0]: b[0],\n",
    "            sample_input[1]: b[1]\n",
    "        })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try dropout in input and output not in recurrent connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "num_nodes = 64\n",
    "keep_prob_train = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # concatenate parameters\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "    output, state = lstm_cell(drop_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  keep_prob_sample = tf.placeholder(tf.float32)\n",
    "  sample_input = []\n",
    "  for _ in range(2):\n",
    "        sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  sample_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  #saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, sample_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.318954 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.63\n",
      "================================================================================\n",
      "mjwy  n vai auhynrpafnj  oesvqyamuij ewcigi soejeqdrllaanta jdh deqqahaq lehxhtze\n",
      "esnagq zstemk cgil seqke cgi ya ylh ijcavise snesmywfndx l fxh nyctfei tyfei cnaz\n",
      "dkrsdp cx ktyue  dmeetceeommqpngfeaz hylp mnclgreb  j oo r neipeojna kossexittsbn\n",
      "fa abgao lvcans ns  eqfmet ayrezsenaaeh t bckontibt yxthvqln  tkd uexxntlxgjzmuig\n",
      "cgzcedllebquvgl jg  aqhahmb i dganyqer ouypcrqc f x  usewntiesz akzyee gaentnj js\n",
      "================================================================================\n",
      "Validation set perplexity: 20.39\n",
      "Average loss at step 100: 2.284526 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 108.32\n",
      "Average loss at step 200: 1.973834 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 134.62\n",
      "Average loss at step 300: 1.882528 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 176.57\n",
      "Average loss at step 400: 1.824494 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 191.81\n",
      "Average loss at step 500: 1.759176 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 200.38\n",
      "Average loss at step 600: 1.762341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 215.42\n",
      "Average loss at step 700: 1.744815 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 214.77\n",
      "Average loss at step 800: 1.726298 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 223.08\n",
      "Average loss at step 900: 1.717484 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 253.65\n",
      "Average loss at step 1000: 1.686760 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "ow between enging a gbspecie and the to the labed called to ides in the added hel\n",
      " x n patricting heir what orgriulation decepius expensix recon n cgntia systempti\n",
      "uup the parting facted thurais of the systewae binland a cented torr tries and wi\n",
      "cbed devene five one t seven zere one five two zero zeo when one footrees binomia\n",
      "jqsair between lxim the appydoes and have zero clowns and natur and n terms the s\n",
      "================================================================================\n",
      "Validation set perplexity: 285.52\n",
      "Average loss at step 1100: 1.698107 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 291.69\n",
      "Average loss at step 1200: 1.688532 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 300.10\n",
      "Average loss at step 1300: 1.692273 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 296.74\n",
      "Average loss at step 1400: 1.661761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 284.46\n",
      "Average loss at step 1500: 1.651604 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 297.57\n",
      "Average loss at step 1600: 1.645614 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 320.16\n",
      "Average loss at step 1700: 1.650068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 334.65\n",
      "Average loss at step 1800: 1.671509 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 279.65\n",
      "Average loss at step 1900: 1.652828 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 311.26\n",
      "Average loss at step 2000: 1.657841 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "vpment machines is grokpnint stron of labearne of madeer demoth jhas that and thi\n",
      "lgor lov one new yer have form of new prool name limeraelism of entremerica  rati\n",
      "utional page even societin calel pristical strucior a had british of leset divide\n",
      "bk iland chare pultanning partra influtionalivy enduction appear to thom hear has\n",
      "mza wornianart charil exember polism four eight rf the a may a guallinad neursh r\n",
      "================================================================================\n",
      "Validation set perplexity: 291.04\n",
      "Average loss at step 2100: 1.652462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 336.32\n",
      "Average loss at step 2200: 1.668264 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 287.96\n",
      "Average loss at step 2300: 1.645068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 305.33\n",
      "Average loss at step 2400: 1.642688 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 312.55\n",
      "Average loss at step 2500: 1.657279 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 332.65\n",
      "Average loss at step 2600: 1.644642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 348.04\n",
      "Average loss at step 2700: 1.628187 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 316.77\n",
      "Average loss at step 2800: 1.618380 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 356.66\n",
      "Average loss at step 2900: 1.624316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 320.83\n",
      "Average loss at step 3000: 1.645047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "ns be baroquage refrese gdp in but and sao touried in the apprace and ternine sou\n",
      "ner for snies a bother in sitles benumences an asia and pofestical gods popular o\n",
      "z number and imutoried accountertayal neditings an externation the recorder for c\n",
      "ffear large torry population western of the bonistry from the politication the ju\n",
      "bkinant in drinks of serio eaver governmentations movemmarrii ans from withorthad\n",
      "================================================================================\n",
      "Validation set perplexity: 331.14\n",
      "Average loss at step 3100: 1.611771 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 358.47\n",
      "Average loss at step 3200: 1.629887 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 339.86\n",
      "Average loss at step 3300: 1.627307 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 309.37\n",
      "Average loss at step 3400: 1.620178 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 337.65\n",
      "Average loss at step 3500: 1.611366 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 320.93\n",
      "Average loss at step 3600: 1.626605 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 328.72\n",
      "Average loss at step 3700: 1.596283 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 355.86\n",
      "Average loss at step 3800: 1.598199 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 367.06\n",
      "Average loss at step 3900: 1.588260 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 349.89\n",
      "Average loss at step 4000: 1.606332 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "hkly and adveriter with immr a in operati arket one km fzne couriculary of liness\n",
      "tmations without what opanicular fez ganks have simply baval and lay one six six \n",
      "mfan one laye or open firorpresmation scafence osca his lig mathematicism evend f\n",
      "kqsoods sugme is eye severo internationa and see one a currests and poidnism will\n",
      "iws a main of some boose ees began failer swortall to mainfives in sixt were as f\n",
      "================================================================================\n",
      "Validation set perplexity: 396.57\n",
      "Average loss at step 4100: 1.622458 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 333.45\n",
      "Average loss at step 4200: 1.599910 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 376.00\n",
      "Average loss at step 4300: 1.568136 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 328.21\n",
      "Average loss at step 4400: 1.596166 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 345.45\n",
      "Average loss at step 4500: 1.577604 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 325.88\n",
      "Average loss at step 4600: 1.589607 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 372.00\n",
      "Average loss at step 4700: 1.603534 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 381.92\n",
      "Average loss at step 4800: 1.599131 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 333.35\n",
      "Average loss at step 4900: 1.612085 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 360.10\n",
      "Average loss at step 5000: 1.623987 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "================================================================================\n",
      "xack of is tendo of irth actual idea dance only languish four six four transe hum\n",
      "qwn of tion one tium for becom titlers be the extences is the spitc eight three a\n",
      "clement r their as two zero stifn i have vilcu right in the woon intour instip co\n",
      "wpathous than of time of ccess in to the hoking imay the colonoming this may call\n",
      "sfunction includine testro confetics to main the finant in europe brown frien sta\n",
      "================================================================================\n",
      "Validation set perplexity: 370.80\n",
      "Average loss at step 5100: 1.587063 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 357.58\n",
      "Average loss at step 5200: 1.587970 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 347.16\n",
      "Average loss at step 5300: 1.571904 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 349.00\n",
      "Average loss at step 5400: 1.562033 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 347.26\n",
      "Average loss at step 5500: 1.558177 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 340.76\n",
      "Average loss at step 5600: 1.541599 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 337.77\n",
      "Average loss at step 5700: 1.579144 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 324.24\n",
      "Average loss at step 5800: 1.565022 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 340.92\n",
      "Average loss at step 5900: 1.570293 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 331.63\n",
      "Average loss at step 6000: 1.534644 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "================================================================================\n",
      "ull must of the phany weal line same showed seal and with site one nine six zero \n",
      "n to becommon his one times in known  and confulmsia legal as and kales was centr\n",
      "my and purnic one one zero known islad sji the facted and persons it is and five \n",
      "qge one five of the claw and rafs ford provies whetted stimate bitmer in indepenr\n",
      "us mountment is is seven nine six zero he time ton secondities of glabsite also m\n",
      "================================================================================\n",
      "Validation set perplexity: 341.31\n",
      "Average loss at step 6100: 1.583989 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 329.11\n",
      "Average loss at step 6200: 1.579262 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 316.72\n",
      "Average loss at step 6300: 1.565217 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 323.41\n",
      "Average loss at step 6400: 1.584094 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 330.54\n",
      "Average loss at step 6500: 1.573693 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 333.63\n",
      "Average loss at step 6600: 1.570658 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 328.51\n",
      "Average loss at step 6700: 1.558216 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 328.08\n",
      "Average loss at step 6800: 1.578058 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 311.09\n",
      "Average loss at step 6900: 1.609438 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 302.12\n",
      "Average loss at step 7000: 1.580415 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "swever and helse and universitions antedom the but kup alves countries other oif \n",
      "zgae in the service reuseuven effective given guance nine eight six for of airs a\n",
      "tfishment for total their he sv vand in launce proqeerstilled represbn hom paller\n",
      "izer may to diets comain inter which k dr where remacinterva other postual group \n",
      "qp such author majuria mansvolures the dily estars sworntinion essait and two teb\n",
      "================================================================================\n",
      "Validation set perplexity: 321.73\n",
      "Average loss at step 7100: 1.571489 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 334.93\n",
      "Average loss at step 7200: 1.552706 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 348.63\n",
      "Average loss at step 7300: 1.553415 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 354.17\n",
      "Average loss at step 7400: 1.571554 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 353.08\n",
      "Average loss at step 7500: 1.562526 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 351.90\n",
      "Average loss at step 7600: 1.546313 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 359.62\n",
      "Average loss at step 7700: 1.566598 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 356.10\n",
      "Average loss at step 7800: 1.551503 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 357.89\n",
      "Average loss at step 7900: 1.565592 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 359.26\n",
      "Average loss at step 8000: 1.554681 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "mv the women stills joyness of zero visites dance are manufails claimed ker crite\n",
      "ievo and merchions to bookible from the heharmine year pub charaphil are honeft a\n",
      "skorized one island scientious musicil telemian the mexmlmated stock one aton a l\n",
      "vvary and blacked higher altrued carest as a chronightes capition topark philosop\n",
      "two an evening spose pressive surrow  rich is support of the kare in peates and n\n",
      "================================================================================\n",
      "Validation set perplexity: 361.74\n",
      "Average loss at step 8100: 1.552256 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 363.25\n",
      "Average loss at step 8200: 1.550776 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 358.39\n",
      "Average loss at step 8300: 1.540616 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 356.46\n",
      "Average loss at step 8400: 1.557086 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 343.08\n",
      "Average loss at step 8500: 1.574730 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 342.66\n",
      "Average loss at step 8600: 1.565836 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 345.37\n",
      "Average loss at step 8700: 1.545205 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 365.71\n",
      "Average loss at step 8800: 1.592020 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 357.63\n",
      "Average loss at step 8900: 1.587120 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 355.00\n",
      "Average loss at step 9000: 1.546664 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "vcrime rocalling that end to empolimaticle five zero one one two zero one nine se\n",
      "fc have is the li only rousually ering five four to tha proqet lands all behind b\n",
      "ped in good an uset wton populaard to dimending the fibations he propertice relse\n",
      "mk specialy neating monthany pictions related assatable hyper res exportan by the\n",
      "cisang mhovers fever danies the dedigity rate on bible conring long adwardene rec\n",
      "================================================================================\n",
      "Validation set perplexity: 366.28\n",
      "Average loss at step 9100: 1.545025 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 361.43\n",
      "Average loss at step 9200: 1.538887 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 362.70\n",
      "Average loss at step 9300: 1.581912 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 356.41\n",
      "Average loss at step 9400: 1.564627 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 364.60\n",
      "Average loss at step 9500: 1.547047 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 371.03\n",
      "Average loss at step 9600: 1.534008 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 372.85\n",
      "Average loss at step 9700: 1.541617 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 368.69\n",
      "Average loss at step 9800: 1.549665 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 368.58\n",
      "Average loss at step 9900: 1.526257 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 362.59\n",
      "Average loss at step 10000: 1.527673 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "cs the  esting this space use only to trymple the ing blenjis s can the air maa c\n",
      "vwspine and ct for more relot ditudi bases and apprious player one nine six eight\n",
      "zvast one nine four and few that experters are dr sumpjefority campative may stan\n",
      "ggen shoold these imffinisco to and morta its of the perthrough use she with and \n",
      "aceneddity is three th celes scalexism would complaii poors pounder flimet incomm\n",
      "================================================================================\n",
      "Validation set perplexity: 377.04\n",
      "Average loss at step 10100: 1.563913 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 377.61\n",
      "Average loss at step 10200: 1.566248 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 378.44\n",
      "Average loss at step 10300: 1.555153 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 376.68\n",
      "Average loss at step 10400: 1.554828 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 376.92\n",
      "Average loss at step 10500: 1.545053 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 378.71\n",
      "Average loss at step 10600: 1.522162 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 378.45\n",
      "Average loss at step 10700: 1.548226 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 377.84\n",
      "Average loss at step 10800: 1.552691 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 377.60\n",
      "Average loss at step 10900: 1.575768 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 374.95\n",
      "Average loss at step 11000: 1.546290 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "n the hosed in remain a popularlow in thinkover gasition b one nine briting ze on\n",
      "mfy cotent controls no the last patro which butched the fields four three kest pa\n",
      "tdn jonespecing some walton a later sults as zero moved itsens of gave howed by a\n",
      "bc d one eight one six zero three nine one zero fudzions titlanal intell of dienc\n",
      "undic wherea for the claim dos servastic mean became cast tel coerch homen allowe\n",
      "================================================================================\n",
      "Validation set perplexity: 377.11\n",
      "Average loss at step 11100: 1.565971 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 374.97\n",
      "Average loss at step 11200: 1.547884 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 375.57\n",
      "Average loss at step 11300: 1.552837 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 375.01\n",
      "Average loss at step 11400: 1.555466 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 373.48\n",
      "Average loss at step 11500: 1.565508 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 372.68\n",
      "Average loss at step 11600: 1.548219 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 372.96\n",
      "Average loss at step 11700: 1.558671 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 374.21\n",
      "Average loss at step 11800: 1.572514 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 374.56\n",
      "Average loss at step 11900: 1.552367 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 375.11\n",
      "Average loss at step 12000: 1.576095 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "upborry of the use and do the studied by regarding between an angle galeindicatin\n",
      "emy good lupon of paptext comes with exicondal in the system of it from practing \n",
      "zrs wroto down securoce it alsillowing the john companyyvdled one nine nine chari\n",
      "df masu routann masiep in terwish or the amount celling pagnian were on physify l\n",
      "ship on parts a republisher may in troad one nine year fore defination and always\n",
      "================================================================================\n",
      "Validation set perplexity: 373.68\n",
      "Average loss at step 12100: 1.565784 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 373.08\n",
      "Average loss at step 12200: 1.560647 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 374.06\n",
      "Average loss at step 12300: 1.561019 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 373.14\n",
      "Average loss at step 12400: 1.547562 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 372.68\n",
      "Average loss at step 12500: 1.553470 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 373.27\n",
      "Average loss at step 12600: 1.565319 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 372.53\n",
      "Average loss at step 12700: 1.554747 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 372.45\n",
      "Average loss at step 12800: 1.548082 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 371.77\n",
      "Average loss at step 12900: 1.566856 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 370.95\n",
      "Average loss at step 13000: 1.564848 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "volution it is before they r e for an all greek coenting north theriting in used \n",
      "uk of natural what the proci and it is various the avaluoime transay two s presen\n",
      "nquenced were one six one two himso traqaity it diche baryth the enothi retained \n",
      "bg the field that phaprent one nine eight in a july is as their britis anthegrote\n",
      "sjit was given to known the resign people witmost of the movemency geneligional t\n",
      "================================================================================\n",
      "Validation set perplexity: 371.13\n",
      "Average loss at step 13100: 1.590058 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 369.11\n",
      "Average loss at step 13200: 1.565228 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 366.81\n",
      "Average loss at step 13300: 1.566157 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 365.97\n",
      "Average loss at step 13400: 1.593787 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 363.53\n",
      "Average loss at step 13500: 1.602624 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 362.42\n",
      "Average loss at step 13600: 1.579899 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 363.18\n",
      "Average loss at step 13700: 1.583313 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 361.93\n",
      "Average loss at step 13800: 1.554964 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 362.20\n",
      "Average loss at step 13900: 1.532491 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 363.79\n",
      "Average loss at step 14000: 1.567096 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.02\n",
      "================================================================================\n",
      " years in liu signified augural short the countment pal derice the enal heharajev\n",
      "wcia straquain boy the two zero c one milkings with the must include pla importan\n",
      "mrt commendings and additions ugh philla often mell as is relation many six plaga\n",
      "qon remlature of fardhile electorysiro for organize of the school bodies of two z\n",
      "qeth a governoe itum to airds help sek there buide other to estates the family bo\n",
      "================================================================================\n",
      "Validation set perplexity: 360.74\n",
      "Average loss at step 14100: 1.564561 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 360.71\n",
      "Average loss at step 14200: 1.567663 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 359.91\n",
      "Average loss at step 14300: 1.577166 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 358.69\n",
      "Average loss at step 14400: 1.563532 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 357.42\n",
      "Average loss at step 14500: 1.562633 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 355.84\n",
      "Average loss at step 14600: 1.549559 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 355.04\n",
      "Average loss at step 14700: 1.565534 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 353.73\n",
      "Average loss at step 14800: 1.578900 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 353.94\n",
      "Average loss at step 14900: 1.577174 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 357.44\n",
      "Average loss at step 15000: 1.570762 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.33\n",
      "================================================================================\n",
      "xc in one nine nine eight to one eight two zero one four the articles emerged spe\n",
      "cranges of brue banneway in pofer the condaranga series on problems phydrawic liv\n",
      " as of century andid the destors peduclinda movement of corni venight a snagoptim\n",
      "bbed they nopending televised in jourit slergasuring as for readay the state fren\n",
      "quaan responside channed woose to be an active much bell munijers of microp carbo\n",
      "================================================================================\n",
      "Validation set perplexity: 357.29\n",
      "Average loss at step 15100: 1.544814 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 357.35\n",
      "Average loss at step 15200: 1.550158 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 357.66\n",
      "Average loss at step 15300: 1.569402 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 357.90\n",
      "Average loss at step 15400: 1.567143 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 357.93\n",
      "Average loss at step 15500: 1.592645 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 357.70\n",
      "Average loss at step 15600: 1.586648 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 357.48\n",
      "Average loss at step 15700: 1.575987 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 357.42\n",
      "Average loss at step 15800: 1.593866 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 357.33\n",
      "Average loss at step 15900: 1.567198 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 357.40\n",
      "Average loss at step 16000: 1.576280 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "mpireven its power for crown of acquity in the most header harge turn loqo bankos\n",
      "fds and numeromary embrake stated idge the ship tillgis os complete the steel to \n",
      "uven were juythor the key shoural appliever and present merizon and precestist fo\n",
      "ys speiens and after appard disolution remaain at came intervation texting meta w\n",
      "ii use of ayant the fashied states the famous outpuths variabsts used by googlato\n",
      "================================================================================\n",
      "Validation set perplexity: 357.61\n",
      "Average loss at step 16100: 1.587813 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 357.69\n",
      "Average loss at step 16200: 1.609945 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 357.64\n",
      "Average loss at step 16300: 1.596326 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 357.50\n",
      "Average loss at step 16400: 1.574211 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 357.44\n",
      "Average loss at step 16500: 1.583086 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 357.47\n",
      "Average loss at step 16600: 1.590831 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 357.54\n",
      "Average loss at step 16700: 1.593905 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 357.44\n",
      "Average loss at step 16800: 1.590816 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 357.58\n",
      "Average loss at step 16900: 1.563277 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 357.48\n",
      "Average loss at step 17000: 1.574005 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "xistoaacurrent it in some social sitein hdocument hand the hogst always to cultur\n",
      "wciagnis descripposeur included rejection withines did one even copulation reach \n",
      "mk text had land opelarate to informa end to does before others w viration tof ro\n",
      "ccomics they better list takensionals flamo proted over critici s romas puriputer\n",
      "hfeations n generally who hel in a damation and in only views g two zero zero zer\n",
      "================================================================================\n",
      "Validation set perplexity: 357.63\n",
      "Average loss at step 17100: 1.592308 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 357.74\n",
      "Average loss at step 17200: 1.584280 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 357.60\n",
      "Average loss at step 17300: 1.570750 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 357.63\n",
      "Average loss at step 17400: 1.594709 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 357.59\n",
      "Average loss at step 17500: 1.598587 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 357.40\n",
      "Average loss at step 17600: 1.552923 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 357.45\n",
      "Average loss at step 17700: 1.554995 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 357.17\n",
      "Average loss at step 17800: 1.576073 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 357.06\n",
      "Average loss at step 17900: 1.582993 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 357.02\n",
      "Average loss at step 18000: 1.554327 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "but forces pieces based using at the mhft music incandomes chaact also were cons \n",
      "ix depited by nalyrace s and relosophy onstercuith of three mearibbion opoloss in\n",
      "bh sapinia annual deby as not some tilliel woun there has made fictor among brown\n",
      "tgeneths style anage raw builts sycodes help example of symphydratmay chiyara emp\n",
      "known outer was and formation certained fuocal durims is the each other five is d\n",
      "================================================================================\n",
      "Validation set perplexity: 357.04\n",
      "Average loss at step 18100: 1.545299 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 357.17\n",
      "Average loss at step 18200: 1.540028 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 357.41\n",
      "Average loss at step 18300: 1.536385 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 357.64\n",
      "Average loss at step 18400: 1.543685 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 357.72\n",
      "Average loss at step 18500: 1.542194 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 357.65\n",
      "Average loss at step 18600: 1.516160 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.88\n",
      "Validation set perplexity: 357.74\n",
      "Average loss at step 18700: 1.520364 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 357.92\n",
      "Average loss at step 18800: 1.542062 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 357.72\n",
      "Average loss at step 18900: 1.533719 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 357.75\n",
      "Average loss at step 19000: 1.516565 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.31\n",
      "================================================================================\n",
      "nka not filmed to into eight fundy trappine books boes open that ki and frequent \n",
      "two emploinited in the mixed anates the beyone three six one of suls including he\n",
      "ophothess of awards added or the s kong nitz of the calist information though wes\n",
      "xmla nhh the sodied in tistemn this is state docwugh nadief cope which inversion \n",
      "ntly chrison to many and markers of the killed and as a froqual was dibsture as t\n",
      "================================================================================\n",
      "Validation set perplexity: 357.86\n",
      "Average loss at step 19100: 1.516499 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 357.95\n",
      "Average loss at step 19200: 1.545544 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 357.89\n",
      "Average loss at step 19300: 1.538427 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 357.81\n",
      "Average loss at step 19400: 1.572002 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 357.41\n",
      "Average loss at step 19500: 1.541707 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 357.36\n",
      "Average loss at step 19600: 1.527208 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 357.43\n",
      "Average loss at step 19700: 1.545669 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 357.53\n",
      "Average loss at step 19800: 1.548880 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 357.61\n",
      "Average loss at step 19900: 1.575845 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 357.53\n",
      "Average loss at step 20000: 1.542562 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "yboydree woods alled do or alated nine cerian in the unife been moral guuring dur\n",
      "eithe crability in music cargely area example is an one prise in states europe an\n",
      "qc the coinisson two which relation fine exammann mage knhard property been as ve\n",
      "pk in the french up rum to called life of the power one seven funny doublewhohpro\n",
      "mnricw ted decreatable eurus the tronductry explote exterminition is the cress to\n",
      "================================================================================\n",
      "Validation set perplexity: 357.77\n",
      "Average loss at step 20100: 1.545657 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 357.78\n",
      "Average loss at step 20200: 1.586188 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 357.79\n",
      "Average loss at step 20300: 1.571398 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 357.82\n",
      "Average loss at step 20400: 1.569559 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 357.83\n",
      "Average loss at step 20500: 1.587288 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 357.84\n",
      "Average loss at step 20600: 1.566439 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 357.87\n",
      "Average loss at step 20700: 1.545970 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 357.89\n",
      "Average loss at step 20800: 1.529881 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 357.91\n",
      "Average loss at step 20900: 1.542058 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 357.94\n",
      "Average loss at step 21000: 1.551290 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.39\n",
      "================================================================================\n",
      "set but many its design china may true decaded florize at republe is nece the tri\n",
      "oaske hp helm as ard aocourshml state to relexual stander and a val great waker r\n",
      "dv  location however it to nocument were al den briogb tradition of the constrola\n",
      "kfive e will piuscer puritude indian a bmne john over over in egion to siet dispu\n",
      "cbiogibath is frome of followings along attempt american eight six contectional m\n",
      "================================================================================\n",
      "Validation set perplexity: 357.95\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 21001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          # feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for i in range(2):\n",
    "            feed.append(random_distribution())\n",
    "          # sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0],\n",
    "                                                 sample_input[1]: feed[1]\n",
    "                                                })\n",
    "            # feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "            sample_input[0]: b[0],\n",
    "            sample_input[1]: b[1]\n",
    "        })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
