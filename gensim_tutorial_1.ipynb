{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"/var/folders/9j/wnpgv9rd1852fcj3qqttm7_40000gn/T\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'and', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees', 'and'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "#This is tiny documents with only 9 sentences, now clean all the stop words and remove word that only appears once.\n",
    "stopwords = set('for a of the end to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stopwords] for document in documents]\n",
    "\n",
    "#Remove words that only appears once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "from pprint import pprint\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(13 unique tokens: [u'and', u'minors', u'graph', u'system', u'trees']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'deerwester.dict')) #Store dictionary for future reference \n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assigned a unique integer id to all words appearing in the corpus with the gensim.corpora.dictionary.Dictionary class. This sweeps across the texts, collecting word counts and relevant statistics. In the end, we see there are thirteen distinct words in the processed corpus, which means each document will be represented by twelve numbers (ie., by a 13-D vector). To see the mapping between words and their ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'and': 9, u'minors': 12, u'graph': 11, u'system': 6, u'trees': 10, u'eps': 8, u'computer': 1, u'survey': 5, u'user': 7, u'human': 2, u'time': 4, u'interface': 0, u'response': 3}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "#Actually convert tokenized document to vector\n",
    "new_doc = \"Human and computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-30 20:20:57,461 : INFO : storing corpus in Matrix Market format to /var/folders/9j/wnpgv9rd1852fcj3qqttm7_40000gn/T/deerwester.mm\n",
      "2017-05-30 20:20:57,465 : INFO : saving sparse matrix to /var/folders/9j/wnpgv9rd1852fcj3qqttm7_40000gn/T/deerwester.mm\n",
      "2017-05-30 20:20:57,468 : INFO : PROGRESS: saving document #0\n",
      "2017-05-30 20:20:57,470 : INFO : saved 9x13 matrix, density=25.641% (30/117)\n",
      "2017-05-30 20:20:57,472 : INFO : saving MmCorpus index to /var/folders/9j/wnpgv9rd1852fcj3qqttm7_40000gn/T/deerwester.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(0, 1), (6, 1), (7, 1), (8, 1)]\n",
      "[(2, 1), (6, 2), (8, 1), (9, 1)]\n",
      "[(3, 1), (4, 1), (7, 1)]\n",
      "[(10, 1)]\n",
      "[(10, 1), (11, 1)]\n",
      "[(9, 1), (10, 1), (11, 1), (12, 1)]\n",
      "[(5, 1), (11, 1), (12, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus_vec = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'deerwester.mm'), corpus_vec)  # store to disk, for later use\n",
    "for c in corpus_vec:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('datasets/mycorpus.txt'):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x10c4e2dd0>\n"
     ]
    }
   ],
   "source": [
    "corpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!\n",
    "print(corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/mycorpus.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ff3264018b50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_memory_friendly\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# load one vector into memory at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-106b8fdfab21>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMyCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/mycorpus.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0;31m# assume there's one document per line, tokens separated by whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'datasets/mycorpus.txt'"
     ]
    }
   ],
   "source": [
    "for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the output is the same as for the plain Python list, the corpus is now much more memory friendly, because at most one vector resides in RAM at a time. Your corpus can now be as large as you want.\n",
    "We are going to create the dictionary from the mycorpus.txt file without loading the entire file into memory. Then, we will generate the list of token ids to remove from this dictionary by querying the dictionary for the token ids of the stop words, and by querying the document frequencies dictionary (dictionary.dfs) for token ids that only appear once. Finally, we will filter these token ids out of our dictionary and call dictionary.compactify() to remove the gaps in the token id series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six import iteritems\n",
    "\n",
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('datasets/mycorpus.txt'))\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist \n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify()\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-30 20:25:13,643 : INFO : loading Dictionary object from /var/folders/9j/wnpgv9rd1852fcj3qqttm7_40000gn/T/deerwester.dict\n",
      "2017-05-30 20:25:13,645 : INFO : loaded /var/folders/9j/wnpgv9rd1852fcj3qqttm7_40000gn/T/deerwester.dict\n",
      "2017-05-30 20:25:13,647 : INFO : loaded corpus index from /var/folders/9j/wnpgv9rd1852fcj3qqttm7_40000gn/T/deerwester.mm.index\n",
      "2017-05-30 20:25:13,649 : INFO : initializing corpus reader from /var/folders/9j/wnpgv9rd1852fcj3qqttm7_40000gn/T/deerwester.mm\n",
      "2017-05-30 20:25:13,651 : INFO : accepted corpus with 9 documents, 13 features, 30 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(9 documents, 13 features, 30 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "dictionary = corpora.Dictionary.load(os.path.join(TEMP_FOLDER, 'deerwester.dict'))\n",
    "corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'deerwester.mm')) # comes from the first tutorial, \"From strings to vectors\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-30 20:25:26,401 : INFO : using serial LSI version on this node\n",
      "2017-05-30 20:25:26,404 : INFO : updating model with new documents\n",
      "2017-05-30 20:25:26,407 : INFO : preparing a new chunk of documents\n",
      "2017-05-30 20:25:26,410 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-05-30 20:25:26,411 : INFO : 1st phase: constructing (13, 102) action matrix\n",
      "2017-05-30 20:25:26,416 : INFO : orthonormalizing (13, 102) action matrix\n",
      "2017-05-30 20:25:26,425 : INFO : 2nd phase: running dense svd on (13, 9) matrix\n",
      "2017-05-30 20:25:26,429 : INFO : computing the final decomposition\n",
      "2017-05-30 20:25:26,431 : INFO : keeping 2 factors (discarding 44.243% of energy spectrum)\n",
      "2017-05-30 20:25:26,517 : INFO : processed documents up to #9\n",
      "2017-05-30 20:25:26,523 : INFO : topic #0(3.402): 0.651*\"system\" + 0.369*\"user\" + 0.308*\"eps\" + 0.238*\"response\" + 0.238*\"time\" + 0.233*\"human\" + 0.220*\"computer\" + 0.212*\"and\" + 0.194*\"survey\" + 0.186*\"interface\"\n",
      "2017-05-30 20:25:26,525 : INFO : topic #1(2.613): 0.603*\"graph\" + 0.510*\"trees\" + 0.440*\"minors\" + 0.308*\"and\" + -0.174*\"user\" + -0.133*\"time\" + -0.133*\"response\" + 0.099*\"survey\" + -0.088*\"computer\" + -0.060*\"interface\"\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.45309173631434096), (1, -0.072100884097923493)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing query structures\n",
    "To prepare for similarity queries, we need to enter all documents which we want to compare against subsequent queries. In our case, they are the same nine documents used for training LSI, converted to 2-D LSA space. But that’s only incidental, we might also be indexing a different corpus altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-30 20:29:53,305 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-05-30 20:29:53,311 : INFO : creating matrix with 9 documents and 2 features\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-30 20:30:17,841 : INFO : saving MatrixSimilarity object under /var/folders/9j/wnpgv9rd1852fcj3qqttm7_40000gn/T/deerwester.index, separately None\n",
      "2017-05-30 20:30:17,844 : INFO : saved /var/folders/9j/wnpgv9rd1852fcj3qqttm7_40000gn/T/deerwester.index\n"
     ]
    }
   ],
   "source": [
    "index.save(os.path.join(TEMP_FOLDER, 'deerwester.index'))\n",
    "#index = similarities.MatrixSimilarity.load(os.path.join(TEMP_FOLDER, 'index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is true for all similarity indexing classes (similarities.Similarity, similarities.MatrixSimilarity and similarities.SparseMatrixSimilarity). Also in the following, index can be an object of any of these. When in doubt, use similarities.Similarity, as it is the most scalable version, and it also supports adding more documents to the index later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.99893081), (1, 0.99663335), (2, 0.99971139), (3, 0.96275038), (4, 0.94880307), (5, -0.061702289), (6, -0.0480875), (7, 0.052598059), (8, 0.12318273)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.99971139), (0, 0.99893081), (1, 0.99663335), (3, 0.96275038), (4, 0.94880307), (8, 0.12318273), (7, 0.052598059), (6, -0.0480875), (5, -0.061702289)]\n"
     ]
    }
   ],
   "source": [
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims) # print sorted (document number, similarity score) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
