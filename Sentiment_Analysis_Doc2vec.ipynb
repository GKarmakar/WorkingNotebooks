{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand feraure representation from text data in the form of words and sentences it needs to be converted to vectors so that mathametical computation can be done on them by any algortithm.\n",
    "the process to convert words to vectors is called word mebeddings. word2vec is a algorithm that uses vectors created from words which then can be used for semantic similarity, sentiment analysis and other type of prediction tasks.\n",
    "in this tutorial we will use word2vec model to make sentiment analysis of cornell IMDB moview review database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We will use Gensim for word2vec. \n",
    "\n",
    "#Import \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working on these datasets.\n",
    "The result is to have five documents:\n",
    "test-neg.txt: 12500 negative movie reviews from the test data\n",
    "test-pos.txt: 12500 positive movie reviews from the test data\n",
    "train-neg.txt: 12500 negative movie reviews from the training data\n",
    "train-pos.txt: 12500 positive movie reviews from the training data\n",
    "train-unsup.txt: 50000 Unlabelled movie reviews\n",
    "Movie reviews will be formatted as sentences - each moview review will be separate sentences.\n",
    "Each sentence will end with a new line - parser will depend on this to identify a new sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec only converts word to vector but doc2vec converts word to vectors and also aggregates words \n",
    "of a sentence into a vector. Also it creates a label for each sentence vector as a special word.\n",
    "So we have to format sentences into\n",
    "[['word1', 'word2', 'word3', 'lastword'], ['label1']]\n",
    "LabeldSentence just does that exactly. But for that we need to convert our corpus into sentences and LabeldLineSentence\n",
    "can do that but for a single file. In reality we have to deal with many files. So we need to write our own LabeledLineSentence\n",
    "function to do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        #make sure that keys are unique\n",
    "        \n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_np, line in enumerate(fin):\n",
    "                    yield LabeleSentence(utils.to_unicode(line).split(), \n",
    "                                         [prefix + '_%s' % item_no])\n",
    "\n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentence_perm(self):\n",
    "        return np.random.permutation(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS', 'train-unsup.txt':'TRAIN_UNS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#doc2vec requires us to provide vocabulary whic are unique words that are used in the corpus.\n",
    "#so we need to build another function that will take output of LabeledLineSentence as\n",
    "#array of sentences and create vocabulary for us.\n",
    "model = Doc2Vec(min_count=1, window = 10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trinakarmakar/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/trinakarmakar/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/Users/trinakarmakar/anaconda2/lib/python2.7/site-packages/gensim/models/word2vec.py\", line 857, in job_producer\n",
      "    sentence_length = self._raw_word_count([sentence])\n",
      "  File \"/Users/trinakarmakar/anaconda2/lib/python2.7/site-packages/gensim/models/doc2vec.py\", line 729, in _raw_word_count\n",
      "    return sum(len(sentence.words) for sentence in job)\n",
      "  File \"/Users/trinakarmakar/anaconda2/lib/python2.7/site-packages/gensim/models/doc2vec.py\", line 729, in <genexpr>\n",
      "    return sum(len(sentence.words) for sentence in job)\n",
      "AttributeError: 'numpy.ndarray' object has no attribute 'words'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now we need to train the model. In each epoch model will be fed with sentence of sentences\n",
    "#selected randomly by sentence_perm function.\n",
    "sentences = list(sentences.sentence_perm())\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs = model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inspecting the model: Lets see what are model returns as similar words for the word \"good\".\n",
    "\n",
    "model.most_simlar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we can also open the hood and see whats are the vector inside the model\n",
    "#This is each of the vector of words and sentences. We can do that using model.syn0.\n",
    "#syn is the output layer of the neural network. But instead of printing vectors of all\n",
    "#the word we want to print vectors of sentences.\n",
    "\n",
    "model['TRAIN_NEG_0']\n",
    "\n",
    "#To avoid rerunning model we can save it\n",
    "model.save('./imdb.d2v')\n",
    "#and load it\n",
    "model = Doc2Vec.load('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying Sentiments:\n",
    "Note that we have total 25,000 vectors of reviews 12,500 of positive and 12500 of negative revoews. We need to create a numpy array with review sentence vectors and labels vectors    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_arrays = np.zeros([12500, 100])\n",
    "training_label = np.zeros([25000])\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_train_pos = 'TRIAN_POS_' + str(i)\n",
    "    prefix_train_neg = 'TARIN_NEG_' + str(i) \n",
    "    train_arrays[i] = model[prefix_train_pos]\n",
    "    train_arrays[12500 + i] = model[prefix_train_neg]\n",
    "    train_labels[i] = 1\n",
    "    train_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training arrays looks like this rows and rows of vectors \n",
    "#representing each sentences\n",
    "\n",
    "print(train_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Similarly for test data\n",
    "testing_arrays = np.zeros([12500, 100])\n",
    "testing_label = np.zeros([25000])\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i) \n",
    "    test_arrays[i] = model[prefix_test_pos]\n",
    "    test_arrays[12500 + i] = model[prefix_test_neg]\n",
    "    test_labels[i] = 1\n",
    "    test_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classification: Train a logistic regression, I am using sklearn\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test classifier's accuracy\n",
    "clf.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
