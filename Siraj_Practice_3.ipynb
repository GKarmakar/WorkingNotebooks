{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import exp, array, random, dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random strating weights\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "New weights after training\n",
      "[[ 0.12025406]\n",
      " [ 0.50456196]\n",
      " [-0.85063774]]\n",
      "[ 0.53002734]\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        #Seed the random number generator so that it will generate same numbers in each run\n",
    "        random.seed(1)\n",
    "        \n",
    "        #We model a single neuron 3 input and one output\n",
    "        #Weight is initialized with 3 X 1 random number with value range [-1,1] and mean = 0\n",
    "        self.weights = 2 * random.random((3,1)) - 1\n",
    "    #Sigmoid function to squash the weighted sum of the input between [0,1]\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "    \n",
    "    #derivative of the sigmoid function defines the gradient direction of the sigmoid curve\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    #Training the model adjusting weights by moving it to the opposite direction of the gradeint\n",
    "    \n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        #Pass the training set through a single layer network\n",
    "        output = self.think(training_set_inputs)\n",
    "        \n",
    "        #calculate the error\n",
    "        error = training_set_outputs - output\n",
    "        #Weight is to be adjusted by error multiplied with gradient dot product with input\n",
    "        adjustements = dot(training_set_inputs.T,  error * self.__sigmoid_derivative(output))\n",
    "        self.weights += adjustements\n",
    "        \n",
    "    #Neuron activated input using sigmoid function\n",
    "    def think(self, inputs):\n",
    "        return self.__sigmoid(dot(inputs, self.weights))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #Initialize a single neuron network parameters\n",
    "    \n",
    "    neural_network = NeuralNetwork()\n",
    "    \n",
    "    print(\"Random strating weights\")\n",
    "    print(neural_network.weights)\n",
    "    #we have 4 observations each having three features\n",
    "    training_set_inputs = array([[0,0,1], [1,1,1], [1,0,1], [0,1,1]])\n",
    "    training_set_outputs = array([[0,1,1,0]]).T\n",
    "    \n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 10000)\n",
    "    \n",
    "    print(\"New weights after training\")\n",
    "    print(neural_network.weights)\n",
    "    #Test the neural network with new situation\n",
    "    \n",
    "    print(neural_network.think([1,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting weights (layer 1)\n",
      "[[-0.6169611   0.24421754 -0.12454452  0.57071717  0.55995162 -0.45481479\n",
      "  -0.44707149  0.60374436  0.91627871  0.75186527 -0.28436546  0.00199025]\n",
      " [ 0.36692587  0.42540405 -0.25949849  0.12239237  0.00616633 -0.9724631\n",
      "   0.54565324  0.76528238 -0.27022803  0.23079236 -0.84923752 -0.26235199]\n",
      " [ 0.8662802   0.30275629 -0.20559484  0.57746029 -0.36632776  0.13619731\n",
      "   0.73825478 -0.12765315  0.60429528 -0.71246635  0.40852194  0.40916262]]\n",
      "Random starting weights (layer 2)\n",
      "[[-0.56241579  0.84973526 -0.11571849  0.81863192 -0.88038155 -0.63142583\n",
      "  -0.90528944  0.34976189  0.18924956  0.06662033 -0.91335187  0.12286616\n",
      "  -0.34066311]\n",
      " [ 0.00593367 -0.77621136  0.21438741  0.13188929 -0.98647188  0.23488342\n",
      "   0.82424577  0.58104827  0.98416293  0.91760352  0.58392827 -0.42949808\n",
      "   0.24983341]\n",
      " [-0.04381241 -0.60864964 -0.2353651  -0.89225263 -0.09670318  0.96400948\n",
      "  -0.7521146  -0.7612382   0.47704611  0.17460727 -0.05673493 -0.78574637\n",
      "  -0.54156287]\n",
      " [ 0.79993039 -0.16649292  0.07170333 -0.98758297 -0.39871659 -0.12621366\n",
      "   0.22429799  0.83639615  0.25147334  0.41199513 -0.70033257  0.49212682\n",
      "   0.66201398]\n",
      " [ 0.26745154 -0.12338024 -0.69485445  0.13681923  0.05644856  0.90285753\n",
      "  -0.03928164  0.00511913  0.07375639  0.63840413 -0.88576872  0.33884349\n",
      "   0.53423326]\n",
      " [ 0.41623072  0.59373437  0.11552166  0.93167306 -0.7056862  -0.940706\n",
      "   0.18778699 -0.7718686   0.9016197  -0.34858517 -0.61276262 -0.0843767\n",
      "   0.84080514]\n",
      " [ 0.75813832 -0.49476849 -0.30398241 -0.63482254  0.8035921   0.41305633\n",
      "   0.45331692  0.80017567  0.5583276   0.19830956 -0.41774951 -0.69720947\n",
      "  -0.32965068]\n",
      " [ 0.31510355 -0.85331491 -0.88998721 -0.35361037  0.18096361  0.70779713\n",
      "  -0.42587515 -0.65386555 -0.73195759  0.98930766 -0.64100426 -0.36490635\n",
      "   0.13658281]\n",
      " [-0.98130285  0.80129724  0.95448286  0.11378936 -0.83045231 -0.33399507\n",
      "   0.45685735 -0.71512925  0.10493788 -0.45391348  0.94899028  0.33557381\n",
      "  -0.48869343]\n",
      " [-0.78337701  0.55236145  0.56495599  0.52320783  0.82880623  0.31724556\n",
      "   0.13673516 -0.59648862  0.39659275  0.90439082  0.77992657  0.98713473\n",
      "   0.63740702]\n",
      " [ 0.09024433 -0.09749189  0.78111438  0.94652958  0.18682266 -0.267851\n",
      "  -0.35381061  0.74284651 -0.56873187  0.46989038 -0.26876183  0.6032052\n",
      "   0.56547118]\n",
      " [ 0.40271076  0.24555317 -0.01263471  0.6810754   0.42419397 -0.11218204\n",
      "  -0.93793028 -0.27352048  0.46144358 -0.04886685 -0.31116606  0.28176087\n",
      "  -0.74758936]]\n",
      "Random starting weights (layer 3)\n",
      "[[-0.65706948]\n",
      " [ 0.47417299]\n",
      " [-0.74594121]\n",
      " [-0.26070025]\n",
      " [ 0.20866801]\n",
      " [-0.79379112]\n",
      " [ 0.60474836]\n",
      " [ 0.89110647]\n",
      " [ 0.95807764]\n",
      " [ 0.76246449]\n",
      " [ 0.25536384]\n",
      " [ 0.86097307]\n",
      " [ 0.44957991]]\n",
      "Random starting weights (layer 1) after training\n",
      "[[-1.91233114  0.60758794 -1.77666849  1.37387438  1.02759511 -1.33478289\n",
      "  -2.03047097 -0.00340852  2.00831974  2.76585832 -1.3865596  -1.82615037]\n",
      " [ 0.26121558  0.36639093  0.01483498 -0.14137995 -0.05878293 -0.74270206\n",
      "   0.37510762  0.77174335 -0.53326778 -0.00877881 -0.64982313 -0.07539108]\n",
      " [ 0.71648694  0.22066019  0.38057973  0.08392702 -0.49752568  0.38238535\n",
      "   0.65426793 -0.10386625 -0.07413242 -1.12467322  0.52818506  0.64103044]]\n",
      "Random starting weights (layer 2) after training\n",
      "[[ 0.28467665  0.59701107  0.53409381  1.38035684 -0.91872311 -0.09937034\n",
      "  -1.46485247  0.06919745  0.00935441 -0.24040396 -1.13156957 -0.7029647\n",
      "  -0.83934234]\n",
      " [-0.19539504 -0.74504665  0.09336366 -0.03965508 -0.98809507  0.11363229\n",
      "   0.96741557  0.56943843  0.97698783  0.77648216  0.82764975 -0.38335543\n",
      "   0.21020662]\n",
      " [ 0.66862639 -0.81234379  0.2829922  -0.41472388 -0.12309845  1.38048138\n",
      "  -1.22855089 -0.96028902  0.34758022 -0.05239503 -0.27912358 -1.45769923\n",
      "  -0.94686543]\n",
      " [ 0.25383503 -0.04764302 -0.29995653 -1.40541839 -0.38790997 -0.43937593\n",
      "   0.59763499  0.91246985  0.30308486  0.32271463 -0.31190865  0.80820932\n",
      "   0.76719734]\n",
      " [-0.24861614 -0.00291613 -1.05986465 -0.24372572  0.07105529  0.59376759\n",
      "   0.30796057  0.10567929  0.14001276  0.59979043 -0.56586006  0.67360121\n",
      "   0.66288128]\n",
      " [ 1.00908206  0.41719249  0.54918959  1.30127845 -0.73151649 -0.57380428\n",
      "  -0.20045956 -0.96265816  0.7802403  -0.55126894 -0.7787624  -0.66779893\n",
      "   0.50026286]\n",
      " [ 1.61085427 -0.74727843  0.34575759 -0.06121338  0.76705235  0.94406224\n",
      "  -0.11361212  0.53021488  0.3846953  -0.10684004 -0.64726134 -1.52352664\n",
      "  -0.83268581]\n",
      " [ 0.19501271 -0.83944643 -0.95786883 -0.43888797  0.18252721  0.62788578\n",
      "  -0.34896133 -0.65076903 -0.7306162   0.85399603 -0.46945039 -0.36664496\n",
      "   0.06247291]\n",
      " [-1.79124276  0.98999314  0.38432376 -0.49292111 -0.80915677 -0.79892381\n",
      "   1.0068192  -0.56526427  0.2051555  -0.49773028  1.44381158  0.8643345\n",
      "  -0.26752499]\n",
      " [-1.9455231   0.84402614 -0.25949799 -0.31198708  0.86284228 -0.37440733\n",
      "   0.92109748 -0.35024989  0.55768486  0.96863787  1.39356112  1.8597513\n",
      "   1.05573619]\n",
      " [ 0.6685944  -0.27299877  1.2080363   1.30999688  0.16214782  0.09187945\n",
      "  -0.73493372  0.55762649 -0.68624038  0.25315728 -0.42090953  0.01750357\n",
      "   0.21373635]\n",
      " [ 1.06381195  0.04820854  0.47534897  1.12532288  0.40025242  0.28826719\n",
      "  -1.38395775 -0.46024541  0.34178275 -0.29382857 -0.50262924 -0.37508354\n",
      "  -1.15788424]]\n",
      "Random starting weights (layer 3) after training\n",
      "[[-3.7567343 ]\n",
      " [ 0.7285214 ]\n",
      " [-1.83690054]\n",
      " [-2.34469343]\n",
      " [-0.01084653]\n",
      " [-1.72127642]\n",
      " [ 2.50269099]\n",
      " [ 0.25438029]\n",
      " [ 0.1518702 ]\n",
      " [ 0.54193154]\n",
      " [ 1.84133486]\n",
      " [ 2.99956046]\n",
      " [ 1.56893361]]\n",
      "\n",
      "Considering new situation [1,0,0] -> ?\n",
      "[ 0.99916371]\n"
     ]
    }
   ],
   "source": [
    "#3 Layer neural network\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        random.seed(1234)\n",
    "        \n",
    "        #setting the number of nodes in layer 2 and layer 3\n",
    "        #more nodes --> more cofidence in prediction(?)\n",
    "        \n",
    "        l2 = 5\n",
    "        l3 = 4\n",
    "        \n",
    "        #assign random weights Format - no. of nodes in previous layer, no. of layer in following layer\n",
    "        self.weights1 = 2 * random.random((3, 12)) -1\n",
    "        self.weights2 = 2 * random.random((12, 13)) -1\n",
    "        self.weights3 = 2 * random.random((13,1)) -1\n",
    "        \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / ( 1+ exp(-x))\n",
    "    \n",
    "    #derivative of the sigmoid function defines the gradient direction of the sigmoid curve\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    #Training the model adjusting weights by moving it to the opposite direction of the gradeint\n",
    "    \n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration_i in range(number_of_training_iterations):\n",
    "            a2 = self.__sigmoid(dot(training_set_inputs, self.weights1))\n",
    "            a3 = self.__sigmoid(dot(a2, self.weights2))\n",
    "            output = self.__sigmoid(dot(a3, self.weights3))\n",
    "        \n",
    "            del4 = (training_set_outputs - output)*self.__sigmoid_derivative(output)\n",
    "            del3 = dot(self.weights3, del4.T)*(self.__sigmoid_derivative(a3).T)\n",
    "            del2 = dot(self.weights2, del3)*(self.__sigmoid_derivative(a2).T)\n",
    "        \n",
    "        #get adjustment (gradients update) for each layer\n",
    "        \n",
    "            adjustments3 = dot(a3.T, del4)\n",
    "            adjustments2 = dot(a2.T, del3.T)\n",
    "            adjustments1 = dot(training_set_inputs.T, del2.T)\n",
    "        \n",
    "            self.weights1 += adjustments1\n",
    "            self.weights2 += adjustments2\n",
    "            self.weights3 += adjustments3\n",
    "        \n",
    "    def forward_pass(self, inputs):\n",
    "        a2 = self.__sigmoid(dot(inputs, self.weights1))\n",
    "        a3 = self.__sigmoid(dot(a2, self.weights2))\n",
    "        output = self.__sigmoid(dot(a3, self.weights3))\n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    neural_network = NeuralNetwork()\n",
    "    \n",
    "    print(\"Random starting weights (layer 1)\")\n",
    "    print(neural_network.weights1)\n",
    "    print(\"Random starting weights (layer 2)\")\n",
    "    print(neural_network.weights2)\n",
    "    print(\"Random starting weights (layer 3)\")\n",
    "    print(neural_network.weights3)\n",
    "    \n",
    "    #The training set\n",
    "    training_set_inputs = array([[0,0,1], [1,1,1], [1,0,1], [0,1,1]])\n",
    "    training_set_outputs = array([[0,1,1,0]]).T\n",
    "    \n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 10000)\n",
    "    \n",
    "    print(\"Random starting weights (layer 1) after training\")\n",
    "    print(neural_network.weights1)\n",
    "    print(\"Random starting weights (layer 2) after training\")\n",
    "    print(neural_network.weights2)\n",
    "    print(\"Random starting weights (layer 3) after training\")\n",
    "    print(neural_network.weights3)\n",
    "    \n",
    "    #test with new input\n",
    "    \n",
    "    print(\"\\nConsidering new situation [1,0,0] -> ?\")\n",
    "    print(neural_network.forward_pass(array([1,0,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
